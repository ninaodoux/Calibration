# Calibration
Calibration methods applied to datasets featuring a highly imbalanced target variable.


This repository focuses on the problem of calibration in machine learning models, specifically in the challenging scenario of highly imbalanced target variables. The goal is to create a benchmarking framework for seven of the most well-known calibration methods, offering a structured comparison of their effectiveness in different data conditions. The project will begin with a detailed exploration of each method, defining their theoretical foundations and assessing their relevance in situations where class distributions are skewed. Evaluation metrics will be carefully selected to ensure a meaningful assessment of calibration performance, acknowledging that traditional metrics may not be well-suited for imbalanced datasets. To facilitate experimentation and ease of use, the repository will include three Python classes that allow for iterative testing across various data scenarios, such as varying sample sizes and degrees of imbalance. Through this systematic approach, the project aims to provide valuable insights for researchers and practitioners working with imbalanced classification problems, contributing to the broader understanding of how calibration impacts model performance in real-world settings.
